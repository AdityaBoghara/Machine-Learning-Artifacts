
# Linear Regression 

## Overview
Linear regression is a **supervised learning** algorithm used to model the relationship between a **continuous dependent variable** and one or more **independent variables** by fitting a linear function to observed data. The goal is to learn parameters that produce predictions with minimal error.

---

## Model Formulation

### Simple Linear Regression
$\hat{y} = \beta_0 + \beta_1 x$

Where:

- $\hat{y}$ = predicted output 
- $x$ = input feature  
- $\beta_0$ = intercept  
- $\beta_1$= slope (coefficient)  

The error for each data point is:
$\varepsilon_i = y_i - \hat{y}_i$

---

## Objective and Cost Function
The objective is to estimate $\beta_0$ and $\beta_1$ such that predictions are as close as possible to the true values. This is achieved by minimizing the **Mean Squared Error (MSE)** cost function:

$J(\beta_0, \beta_1) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2$

The cost function is **convex** for linear regression, which guarantees a **single global minimum**.

---

## Gradient Descent Optimization
Gradient descent is an **iterative optimization algorithm** used to minimize the cost function by updating parameters in the direction of the negative gradient.

### Gradients
$\frac{\partial J}{\partial \beta_0} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)$

$\frac{\partial J}{\partial \beta_1} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)x_i$

### Update Rules
$\beta_0 := \beta_0 - \alpha \frac{\partial J(\beta_0, \beta_1)}{\partial \beta_0}$

$\beta_1 := \beta_1 - \alpha \frac{\partial J(\beta_0, \beta_1)}{\partial \beta_1}$

Where:
- $\alpha$ = learning rate (controls step size)

This process is repeated until the algorithm converges.

---

## Convergence Function (Gradient Descent)

### Definition
The **convergence function** defines how parameters are updated iteratively until the cost function reaches its minimum.

### General Update Rule
$\beta_j := \beta_j - \alpha \frac{\partial J(\beta)}{\partial \beta_j}$

---

## Convergence Process
1. Initialize parameters $\beta$
2. Compute predictions and cost $J(\beta)$
3. Compute gradients
4. Update parameters
5. Repeat until convergence

---

## When Convergence Is Achieved
Convergence occurs when:
- The change in the cost function becomes negligible, or  
- Parameter updates approach zero  

At this point, the model has reached the **global minimum** of the cost function.

---

## Intuition
- Gradient descent moves parameters toward the **steepest decrease** in error  
- The learning rate controls **speed vs. stability**
  - Too large → overshooting or divergence  
  - Too small → slow convergence  

---

## Final Outcome
At convergence:
- The cost function is minimized  
- Parameters are stable  
- The best-fit line (or hyperplane for multiple features) is obtained  

This approach scales efficiently to large datasets and extends naturally to **multiple linear regression** using vectorized operations.


